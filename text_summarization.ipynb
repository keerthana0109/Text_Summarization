{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthana0109/Text_Summarization/blob/main/text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#<font face=\"Trebuchet MS\" size=\"6\">\"Text summarization-Using python\"<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Keerthana</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/keerthana0109\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "# Mini-Project"
      ],
      "metadata": {
        "id": "f-lAEnoPkiyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
        "article = scraped_data.read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    article_text += p.text"
      ],
      "metadata": {
        "id": "MgKVRIKpWSFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "metadata": {
        "id": "VedTs0czWVq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ],
      "metadata": {
        "id": "5d9lkvrFWYMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "2yphydgSWlCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwJ6HLdtWuSS",
        "outputId": "164a6d5a-050e-4dfc-8840-00b11fea25d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = sent_tokenize(article_text)"
      ],
      "metadata": {
        "id": "WewpEqFRWcL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQGKH84LXovL",
        "outputId": "8013d531-acc8-41ac-b3fd-df77a27677b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords1 = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "V6vzC971XXwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}\n",
        "for word in word_tokenize(formatted_article_text):\n",
        "    if word not in stopwords1:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1"
      ],
      "metadata": {
        "id": "oIhYPu1BW1N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "metadata": {
        "id": "1zzpZL_KXAdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_list:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "metadata": {
        "id": "lXPUJSAqXw1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "summary = ' '.join(summary_sentences)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppw4vnJjX0pt",
        "outputId": "06930952-740f-47db-9807-460dc68af259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. The artificial intelligence algorithms will also be used to further improve diagnosis over time, via an application of machine learning called precision medicine. A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others. [r] AI founder John McCarthy said: \"Artificial intelligence is not, by definition, simulation of human intelligence\". A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. The main agenda for these scientific diplomacy efforts is to bolster research on artificial intelligence and how it can be utilized in cybersecurity research, development, and overall consumer trust.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Let's we check the original and summarized text value\n",
        "\n",
        "print(\"the original len of the text is: \",len(article_text))\n",
        "\n",
        "print(\"the len of the summarized text is: \",len(summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "_HD2ISwOh4FB",
        "outputId": "191b6c3a-055c-46df-84d9-f214bf280b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the original len of the text is:  70340\n",
            "the len of the summarized text is:  1155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarized text - Output**\n",
        "\n",
        " Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. \n",
        " The artificial intelligence algorithms will also be used to further improve diagnosis over time, via an application of machine learning called precision medicine. \n",
        " A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. \n",
        " Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others. \n",
        " [r] AI founder John McCarthy said: \"Artificial intelligence is not, by definition, simulation of human intelligence\". \n",
        " A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. \n",
        " The main agenda for these scientific diplomacy efforts is to bolster research on artificial intelligence and how it can be utilized in cybersecurity research, development, and overall consumer trust.:\n"
      ],
      "metadata": {
        "id": "dHL0F4zBYUts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **let's we convert this into a function:**"
      ],
      "metadata": {
        "id": "bLbHZpoeZBe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import heapq\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bykQs9wUZrPb",
        "outputId": "186b476e-f5c5-4b0a-d309-6c7319f807c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarization(link):\n",
        "  scraped_data = urllib.request.urlopen(link)\n",
        "  article = scraped_data.read()\n",
        "  parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "  paragraphs = parsed_article.find_all('p')\n",
        "  article_text = \"\"\n",
        "\n",
        "  for p in paragraphs:\n",
        "      article_text += p.text\n",
        "\n",
        "  article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "  article_text = re.sub(r'\\s+', ' ', article_text)\n",
        "  formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
        "  formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
        "\n",
        "  sentence_list = sent_tokenize(article_text)\n",
        "  stopwords1 = set(stopwords.words(\"english\"))\n",
        "  word_frequencies = {}\n",
        "  for word in word_tokenize(formatted_article_text):\n",
        "      if word not in stopwords1:\n",
        "          if word not in word_frequencies.keys():\n",
        "              word_frequencies[word] = 1\n",
        "          else:\n",
        "              word_frequencies[word] += 1\n",
        "\n",
        "  maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "  for word in word_frequencies.keys():\n",
        "      word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "\n",
        "  sentence_scores = {}\n",
        "  for sent in sentence_list:\n",
        "      for word in nltk.word_tokenize(sent.lower()):\n",
        "          if word in word_frequencies.keys():\n",
        "              if len(sent.split(' ')) < 30:\n",
        "                  if sent not in sentence_scores.keys():\n",
        "                      sentence_scores[sent] = word_frequencies[word]\n",
        "                  else:\n",
        "                      sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "\n",
        "\n",
        "  summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "  summary = ' '.join(summary_sentences)\n",
        "  return(summary)"
      ],
      "metadata": {
        "id": "bXo7KpzkZ_yL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Paste the link u need to summarize\n",
        "#@markdown Hint: U can use the link such as wikipedia, scholarpedia etc...\n",
        "link = 'https://en.wikipedia.org/wiki/Doraemon' #@param {type:\"string\"}\n",
        "\n",
        "print(\"Summarized text: \\n\")\n",
        "summarization(link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "9XXsmRMhc8yT",
        "outputId": "322399f7-eccf-40b0-ab65-24647e4e8c99"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized text: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Unlike the anime and manga series, the films are more action-adventure oriented, taking the familiar characters of Doraemon and placing them in a variety of exotic and perilous settings. The 2005 and 2006 surveys conducted by TV Asahi found the Doraemon anime ranked fifth and third, respectively, among the 100 most favorite anime series of all time. As of 2019, it has sold over 250 million copies worldwide, becoming one of the best-selling manga series in history. In 1982, it received the first Shogakukan Manga Award for children\\'s manga. During Doraemon\\'s development, Fujio did not express a change in characters; he said, \"When a manga hero become a success, the manga suddenly stops being interesting. After the 2011 TÅhoku earthquake and tsunami, Shogakukan released an earthquake survival guidebook, which included the main cast of the Doraemon manga series. On the manga\\'s 50th anniversary, an op-ed published on Asahi Shimbun stated that the manga \"has already become a contemporary classic\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}